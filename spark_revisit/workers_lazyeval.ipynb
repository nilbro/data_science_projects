{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkContext - number of workers and lazy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of number of workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is recommended to have one worker per core of the machines. But it can be more or less. Lets check what effect does the number of workers have on the computation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 executors, time = 0.8812801837921143\n",
      "2 executors, time = 1.0833406448364258\n",
      "3 executors, time = 1.0433084964752197\n",
      "4 executors, time = 1.0877187252044678\n"
     ]
    }
   ],
   "source": [
    "for j in range(1,5):\n",
    "    sc= SparkContext(master = \"local[%d]\"%(j))\n",
    "    t0=time()\n",
    "    for i in range(10):\n",
    "        sc.parallelize([1,2]*10000).reduce(lambda x,y:x+y)\n",
    "    print(f\"{j} executors, time = {time()-t0}\")\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the time taken by one worker is almost double than that of time taken by 2 workers. after that there is no significant change in the time, since the code is running on this local machine with 2 cores. If the machine had 4 cores, we could have seen improvement in time till 4 workers. After that it will eventually flatten out. Even the time taken may increase due to context switching between multiple workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy evaluation or call-by-name is an evaluation strategy which delays the evaluation of an expression until its value is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Spark there are two types of operations \n",
    "1. Transformations - Creation of a RDD from another RDD. For example map()--> applies a function to each element of the RDD and creates a new RDD, Or filter()--> applies a filter to each element of the RDD and creates a new RDD with filtered elements\n",
    "2. Actions - These are operations where a non-RDD is created from a RDD. For example count()-->gives a count of all elements in the RDD. The output is not a RDD. Or top(n)--> gives the top n elements of a RDD. Here again the output is not a RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since Spark is optimized for BigData datasets, it will not evaluate a RDD for transformations until an action is performed. The series of transformations are maintained by DAG (Directed Acyclic Graph) which is used by Spark to compute an efficient method to compute all the transformations at once. So when an action is performed, all the previous transformation operations are evaluated in a efficient method for the first time and then the action is performed. This type of evaluation is known as Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The series of transformations are maintained as a RDD lineage which can be seen by the function toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop()\n",
    "sc= SparkContext(master = \"local[2]\")\n",
    "# create a sample list\n",
    "my_list = [i for i in range(1,10)]# parallelize the data\n",
    "rdd_0 = sc.parallelize(my_list)\n",
    "rdd_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add  4 to each element of the rdd. check the lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:53\n",
      "b'(2) PythonRDD[1] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []'\n"
     ]
    }
   ],
   "source": [
    "# add value 4 to each number\n",
    "rdd_1 = rdd_0.map(lambda x : x+4)\n",
    "# RDD object\n",
    "print(rdd_1)\n",
    "# get the RDD Lineage \n",
    "print(rdd_1.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add 20 to each element of the rdd. check the lineage again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[2] at RDD at PythonRDD.scala:53\n",
      "b'(2) PythonRDD[2] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 []'\n"
     ]
    }
   ],
   "source": [
    "# add value 20 each number\n",
    "rdd_2 = rdd_1.map(lambda x : x+20)\n",
    "# RDD Object\n",
    "print(rdd_2)\n",
    "# get the RDD Lineage\n",
    "print(rdd_2.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### there is no change in the lineage because Spark knows adding 24 once is more efficient than adding 4 first and then 24. This is how Spark automatically defines the best path to perform any action and only perform the transformations when required. This method of evaluation is known as Lazy Evaluation. It is specifically helpful for computation of a lot of data, BigData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
